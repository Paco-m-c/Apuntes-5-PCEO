\documentclass[openany]{book}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage[hypertexnames=false]{hyperref}
\usepackage{amstext} 
\usepackage{array}   
\newcolumntype{C}{>{$}c<{$}} 


\input{structure}
\usepackage{geometry}
\geometry{
    top=3cm,
    bottom=3cm,
    left=3cm,
    right=3cm,
    headheight=14pt, 
    footskip=1.4cm,
    headsep=10pt,
}
\usepackage{graphicx}
\title{Apuntes de Inferencia Estadística}
\author{Paco Mora}
\date{\today}

\begin{document}

Mirar los apuntes de Lorencio

% \maketitle

% \chapter{Tema 2}

% \begin{exercise}
%     \textbf{Ejercicio 1.a) y 1.b)}

%     Los valores que puede tomar el vector son
%     $$ \{(0,0,0),(1,0,0),(0,1,0),(0,0,1),(1,1,0),(1,0,1),(0,1,1),(1,1,1)\} $$

%     Donde tenemos que:
%     $$ P(X_1=0,X_2=0,X_3=0) = P(X=0)^3 = (1-p)^3 $$
%     $$ P(X_1=1,X_2=0,X_3=0) = p(1-p)^2 = P(X_1=0,X_2=1,X_3=0) = P(X_1=0,X_2=0,X_3=1)$$
%     $$ P(X_1=1,X_2=1,X_3=0) = p^2(1-p) = P(X_1=1,X_2=0,X_3=1) = P(X_1=0,X_2=1,X_3=1) $$
%     $$ P(X_1=1,X_2=1,X_3=1) = p^3 $$

%     Es fácil comprobar que la suma de todas las probabilidades es 1.

%     Obtendremos la media muestral:
%     $$ \overline{X} = \dfrac{X_1+X_2+X_3}{n} $$ 

%     Que tomará los valores $ \left\{0,\dfrac{1}{3},\dfrac{2}{3},1\right\} $, calculamos sus probabilidades:
%     $$ P(\overline{X} = 0) = P(X_1=0,X_2=0,X_3=0) = (1-p)^3 $$

%     De forma análoga sacamos
%     $$ P\left(\overline{X}=\dfrac{1}{3}\right) = 3p(1-p)^2 $$
%     $$ P\left(\overline{X}=\dfrac{2}{3}\right) = 3p^2(1-p) $$
%     $$ P\left(\overline{X}=1\right) = p^3 $$


% \end{exercise}

% \begin{exercise}
%     $  $

%     Dada una m.a.s $ (X_1,X_2,...,X_n)  $ de $ X \sim Exp(d) $, obtener la distribución en el muestreo del estadístico $ S = \sum\limits_{j=1}^{n}X_j $. Intentamos ver si Exp es reproductiva:
%     $$ \phi_{S}(t) = \phi_{X}(t)^{n} = \left( \left( 1-\dfrac{it}{\alpha}    \right)^{-1} \right)^{n} = \left( 1-\dfrac{it}{\alpha} \right)^{-n} $$

%     No lo es, pero si lo comparamos con la distribución Gamma, obtenemos que $ Exp(\alpha) \equiv \gamma(a = \alpha, p=1) $

%     Vamos a ver si la distribución Gamma es reproductiva respecto a algún parámetro.

%     $$ \phi_{S}(t) = \left( 1-\dfrac{it}{a} \right)^{-np} \implies S \sim \gamma(a,np) $$

%     Luego Gamma es reproductiva, entonces tenemos que el estadístico es:
%     $$ S = \sum\limits_{j=1}^{n}X_j \sim \gamma(\alpha, n) $$
% \end{exercise}


% \begin{exercise}
%     $  $

%     $$ \phi_{X}(t) = e^{it\mu-t^2\dfrac{\sigma^2}{2}} $$

%     $$ S = \sum\limits_{j=1}^{n}X_j \implies \phi_{S}(t) = \prod_{j=1}^{n}\phi_{X_j}(t) = \phi_{X}(t)^{n} = \left( e^{it\mu-t^2\dfrac{\sigma^2}{2}} \right)^{n} = e^{it\mu n-t^2 \dfrac{\sigma^2n}{2}} $$

%     Luego tenemos $ S \sim N(n\mu,n\sigma^2) $

%     Si queremos sacar el estadístico $ \overline{X}=\dfrac{S}{n} $ haremos:
%     $$ \phi_{\overline{X}}(t) = E(e^{it\overline{X}}) = E(e^{itS/n}) = \phi_{S}\left(\dfrac{t}{n}\right) = e^{it\mu-\dfrac{t^2\sigma^2n}{2n^2}}  $$
%     $$ \overline{X}\sim N\left(t,\dfrac{\sigma^2}{n}\right) $$
% \end{exercise}

% \begin{exercise}
%     $  $

%     Obtenemos primero la distribución de $ Y = X^2 $ utilizando cambio de variable:
%     $$ x(y) = \sqrt{y}\hspace{5mm}x'(y) = \dfrac{1}{2}y^{-1/2} $$
%     $$ f_{Y}(y) = \cancel{2y^{1/2}}e^{-y/\theta}\cancel{\dfrac{1}{2}y^{-1/2}}I_{(0,+\infty)}(y)=\dfrac{1}{\theta}e^{-y/\theta}I_{(0,+\infty)} \implies$$
%     $$ \implies Y \sim Exp\left(\dfrac{1}{\theta}\right) \equiv \gamma\left(\dfrac{1}{\theta},1\right) $$

%     Para ver la media y la varianza basta consultarlo en la hoja para la distribución Gamma.
% \end{exercise}

% \begin{proposition}
%     Para la media muestral tenemos:
%     $$ E(\overline{X}) = E(X) $$
% \end{proposition}
% \begin{demonstration}
%  $$   E(\overline{X}) = E(\sum\limits_{j=1}^{n}\dfrac{X_j}{n}) = \dfrac{1}{n}\sum\limits_{j=1}^{n}E(X_j) = \dfrac{1}{n}nE(X) = E(X)$$
% \end{demonstration}

% \begin{proposition}
%     Para la media muestral tenemos:
%     $$ Var(\overline{X}) = \dfrac{Var(X)}{n} $$
% \end{proposition}

% \begin{demonstration}
%     $$ Var(\overline{X}) = Var\left(\sum\limits_{j=1}^{n}X_j \right) = \dfrac{1}{n^2}\sum\limits_{j=1}^{n}Var(X_j) = \dfrac{1}{n^2}nVar(X) = \dfrac{Var(X)}{n} $$
% \end{demonstration}

% \begin{proposition}
%     $$ \overline{X} \xrightarrow{P}E(X) $$

% \end{proposition}
% \begin{demonstration}
%     Recordemos que decimos que una sucesión converge en probabilidad si se cumple una de las siguientes condiciones equivalentes:
%     $$ \forall \varepsilon >0, \lim_{n \to \infty} P(|X_n-X|\leq  \varepsilon) = 1 $$
%     $$ \forall \varepsilon >0, \lim_{n \to \infty} P(|X_n-X|\geq  \varepsilon) = 0 $$

%     Usaremos también la desigualdad de Tchebychev:
%     $$ \forall  \varepsilon>0, P(|\overline{X}-E(\overline{X})|\geq \varepsilon) \leq  \dfrac{Var(\overline{X})}{\varepsilon^2} $$
    
%     Aplicándolo a nuestro caso:
%     $$ \forall  \varepsilon>0, P(|\overline{X}-E(X)|\geq \varepsilon) \leq  \dfrac{Var(X)}{n\varepsilon^2} \to 0 $$


% \end{demonstration}

% \begin{proposition}
%     $$ \dfrac{\overline{X}-E(X)}{\sqrt{\dfrac{Var(X)}{n}}}\to_{d}N(0,1) $$
% \end{proposition}

% \begin{demonstration}
%     Basta usar el teorema central del límite que repasamos en el Tema 1.
% \end{demonstration}

% \begin{proposition}
%     \textbf{Propiedades de la proporción muestral}

%     \begin{enumerate}
%         \item $ \sum\limits_{j=1}^{n}X_j = n\widetilde{p} \sim B(n,p) $
%         \item $ E(\widetilde{p}) = p $
%         \item $ Var(\widetilde{p}) = \dfrac{p(1-p)}{n} $
%         \item $ \widetilde{p}\to_{p}p $
%         \item $ \dfrac{\widetilde{p}-p}{\sqrt{\dfrac{p(1-p)}{n}}} \to_{d}N(0,1) $

%     \end{enumerate}

% \end{proposition}
% \begin{demonstration}
%     Todas se derivan de la Observación 1 del guion.
% \end{demonstration}

% \textbf{Observación sobre la funcion de distribución empírica}

% Si $ A = \{X_j \leq  n\} $
% $$ P(A) = P(X_j\leq n ) = P(X\leq n) = F(n) $$

% Con esta observación demostramos las siguientes propiedades:


\section*{Teorema de Wald}

 \subsection*{Consistencia}
 Sea $ X \sim F(\cdot ,\theta),\ \theta \in \Theta, \hat{\theta}_n $ estimador de $ \theta\ \forall n \in \mathbb{N} $, definimos las siguientes operaciones:
 $$ P_{\theta_0}(\hat{\theta}_n \in A ) := P(\hat{\theta}_n\in A| \theta = \theta_0) $$
$$ E_{\theta_0}[\hat{\theta}_n] = \idotsint\limits_{\psi}^{}\hat{\theta}_n(x)L(x,\theta_0)dx $$

Se dice que $ \hat{\theta}_n $ es consistente para $ \theta \in \Theta $ si $ \hat{\theta}_n\xrightarrow{P_{\theta_0}} \theta_0\ \text{(convergencia en probabilidad)},\ \forall \theta_0 \in \Theta $

% \begin{flushright}
%     \textbf{Observación}
% \end{flushright}

\begin{proposition}
    Si $ \Theta $ es finito y $ \hat{\theta}_n $ es estimador de $ \theta $ entonces se da la consistencia del estadístico si y solo si:
    $$ \lim_{n \to \infty}P_{\theta_0}(\hat{\theta}_n=\theta_0)=1,\ \forall \theta_0 \in \Theta $$
    
    Es decir,
    $$ \hat{\theta}_{n} \xrightarrow{P_{\theta_0}} \theta_0\ \forall \theta \in \Theta \iff \lim_{n \to \infty}P_{\theta_0}(\hat{\theta}_{n}=\theta_0 )=1$$
    
\end{proposition}

\begin{demonstration}
    $  $\\
    $ \impliedby $

    Trivial.\\
    $ \implies $

    Si $ \hat{\theta}_{n} $ es el EMV de $ \theta $, tomará un valor aislado dentro de $ \Theta $.
    
    Si $ \hat{\theta}_{n} \xrightarrow{P_{\theta_0}} \theta_0 $, tomando $ \varepsilon $ suficientemente pequeño, $ \hat{\theta}_{n} $ solo puede tomar el valor de $ \theta_0   $, luego $ \lim_{n \to \infty}P_{\theta_0}(\hat{\theta}_{n} = \theta_0) = 1 $


\end{demonstration}

\begin{proposition}
    \textbf{Desigualdad de Jensen}

    Sea $ X $ v.a. y $ g $ una función cóncava, entonces $ E[g(X)]<E(E[X]) $ siempre que las esperanzas anteriores existan.
\end{proposition}

\begin{proposition}
    \textbf{Ley fuerte de Kolmogorov}

    Sea $ \{X_n\}_{n=1}^{\infty} $ sucesión de v.v.a.a. independientes, idénticamente distribuidas y con media finita. Entonces:
    $$ \sum\limits_{i=1}^{n} \dfrac{X_i}{n} \xrightarrow{c.s.} E[X] $$
\end{proposition}

\begin{lemma}
    Sean $ \{A_n\},\ \{B_n\} $ con $ \lim_{n \to \infty}\{P(A_n)\} = \lim_{n \to \infty} \{P(B_n)\} = 1 $, entonces:
    $$ \lim_{n \to \infty} P(A_n \cap B_n) = 1 $$
\end{lemma}

\begin{demonstration}
    $$ P(A_n \cap B_n) = P(A_n) + P(B_n) - P(A_n \cup B_n) $$
    
    Pero tenemos:
    $$ 1 \geq\ P(A_n\cup B_n) \geq P(A_n) \to 1 $$
    
    Luego $ \lim_{n \to \infty}P(A_n \cap B_n) = 1 $ y tenemos que:
    $$ P(A_n \cap B_n) = P(A_n) + P(B_n) - P(A_n \cup B_n) = 1 +1-1 =1 $$

\end{demonstration}


\begin{theorem}
    Sea $ X $ variable aleatoria con función de distribución $ F(\cdot ,\theta) $ para $ \theta \in \Theta $, siendo $ \Theta $ un conjunto infinito. Supongamos que se verifica:
    \begin{itemize}
        \item (A1) El soporte de $ F(\cdot ,\theta) $ es común para todo $ \theta \in \Theta $.
        \item (A2) $ E_{\theta_0}\left[\log{\dfrac{f}{X,\theta}}\right] $ existe y es finita para todo $ \theta,\ \theta_0 \in \Theta$
        
    \end{itemize}
    Si para todo $ n \in \mathbb{N} $ y $ (X_1,...,X_n) $ m.a.s de $ X $ existe el $ \operatorname{EMV}(\hat{\theta}_{n}) $ de $ \theta $ y es único entonces $ \hat{\theta}_{n} $ es un estimador consistente del parámetro $ \theta $.
\end{theorem}
\begin{demonstration}
    Utilizaremos la Desigualdad de Jensen, tomaremos $ g = \log{} $ y la v.a. $ \dfrac{f(X,\theta)}{f(X,\theta_0)} $. La Desigualdad de Jensen entonces nos dice:
    $$ E_{\theta_0}\left[\log{\left(\dfrac{f(X,\theta)}{f(X,\theta_0)}\right)}\right] < \log{\left(  E_{\theta_0}\left[\dfrac{f(X,\theta)}{f(X,\theta_0)}\right]\right)} $$

    Pero notemos que:
    $$   E_{\theta_0}\left[\dfrac{f(X,\theta)}{f(X,\theta_0)}\right] = \int\limits_{}^{} \dfrac{f(x,\theta)}{f(x,\theta_0)}f(x,\theta_0)dx =_{(A1)} 1 $$

    Luego tenemos que:
    $$ E_{\theta_0}\left[\log{\left(\dfrac{f(X,\theta)}{f(X,\theta_0)}\right)}\right] < 0 $$

    Usaremos ahora la ley fuerte de Kolmogorov a la sucesión $ \left\{\log{\left(  \dfrac{f(X_n,\theta)}{f(X_n,\theta_0)}\right)}\right\}_{n} $ y utilizando que la convergencia casi segura es más fuerte que la convergencia en probabilidad, tenemos :
    $$\dfrac{\log{\left( \dfrac{L(\mathbb{X},\theta)}{L(\mathbb{X},\theta_0)} \right)}}{n} =  \sum\limits_{i=1}^{n} \dfrac{\log{\left( \dfrac{f(x_i,\theta)}{f(x_i,\theta_0)} \right)}}{n} \xrightarrow{P_{\theta_0}} E_{\theta_0}\left[\log{\left(\dfrac{f(X,\theta)}{f(X,\theta_0)}\right)}\right] < 0 $$

    Como a partir de cierto $ n $ el logaritmo de la izquierda será negativo podemos tomar:
    $$ \lim_{n \to \infty}P_{\theta_0} \left( \log{ \dfrac{L(\mathbb{X},\theta)}{L(\mathbb{X},\theta_0  )}}<0 \right) = \lim_{n \to \infty}P_{\theta_0} (L(\mathbb{X},\theta)<L(\mathbb{X},\theta_0)) = 1$$

    Como el espacio paramétrico es finito, utilizaremos la equivalencia para la definición de consistencia y buscaremos el límite:
    $$ \lim_{n \to \infty} P_{\theta_0}(\hat{\theta}_{n} = \theta_0 ) $$

    Estudiamos ahora el suceso $ \{\hat{\theta}_{n}(x) = \theta_0\} = \bigcap _{\substack{\theta \in \Theta\\\theta\ne \theta_0}} \{L(\mathbb{X},\theta) < L(X,\theta_0)\} $

    El suceso es intersección finita de sucesos cuya probabilidad tienen límite 1. Utilizando el lema llegamos a:
    $$ \lim_{n \to \infty}P(\hat{\theta}_{n} = \theta_0 ) = 1 $$
\end{demonstration}


\begin{example}
    $ X \sim \mathcal{U}(\theta,\theta+1),\ \theta \in \mathbb{R},\ X_1,...,X_n$ m.a.s de $ X $, $ (f(x,\theta) = 1,\ si\ x \in (\theta,\theta+1)) $. Buscar el EMV de $ \theta $.

    $$ L(\mathbb{X},\theta) = 1\ si\ (x_i \in (\theta,\theta+1),\ i = 1,...,n) \iff (\theta < x_{1:n}\ y\ x_{n:n}< \theta+1) \iff (x_{n:n}-1 < \theta < x_{1:n}) $$

    En $ \alpha (x_{n:n}-1)+(1-\alpha)x_{1:n} $ se alcanza el máximo de $ L(\mathbb{X},\theta)\ \forall\ \alpha \in (0,1) $
\end{example}



\end{document}