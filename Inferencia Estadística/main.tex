\documentclass[openany]{book}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage[hypertexnames=false]{hyperref}
\usepackage{amstext} 
\usepackage{array}   
\newcolumntype{C}{>{$}c<{$}} 


\input{structure}
\usepackage{geometry}
\geometry{
    top=3cm,
    bottom=3cm,
    left=3cm,
    right=3cm,
    headheight=14pt, 
    footskip=1.4cm,
    headsep=10pt,
}
\usepackage{graphicx}
\title{Apuntes de Inferencia Estadística}
\author{Paco Mora}
\date{\today}
\setcounter{secnumdepth}{0}

\begin{document}
\maketitle

\noindent Estos apuntes están hechos como un complemento a los apuntes de Lorencio.

% \maketitle

% \chapter{Tema 2}

% \begin{exercise}
%     \textbf{Ejercicio 1.a) y 1.b)}

%     Los valores que puede tomar el vector son
%     $$ \{(0,0,0),(1,0,0),(0,1,0),(0,0,1),(1,1,0),(1,0,1),(0,1,1),(1,1,1)\} $$

%     Donde tenemos que:
%     $$ P(X_1=0,X_2=0,X_3=0) = P(X=0)^3 = (1-p)^3 $$
%     $$ P(X_1=1,X_2=0,X_3=0) = p(1-p)^2 = P(X_1=0,X_2=1,X_3=0) = P(X_1=0,X_2=0,X_3=1)$$
%     $$ P(X_1=1,X_2=1,X_3=0) = p^2(1-p) = P(X_1=1,X_2=0,X_3=1) = P(X_1=0,X_2=1,X_3=1) $$
%     $$ P(X_1=1,X_2=1,X_3=1) = p^3 $$

%     Es fácil comprobar que la suma de todas las probabilidades es 1.

%     Obtendremos la media muestral:
%     $$ \overline{X} = \dfrac{X_1+X_2+X_3}{n} $$ 

%     Que tomará los valores $ \left\{0,\dfrac{1}{3},\dfrac{2}{3},1\right\} $, calculamos sus probabilidades:
%     $$ P(\overline{X} = 0) = P(X_1=0,X_2=0,X_3=0) = (1-p)^3 $$

%     De forma análoga sacamos
%     $$ P\left(\overline{X}=\dfrac{1}{3}\right) = 3p(1-p)^2 $$
%     $$ P\left(\overline{X}=\dfrac{2}{3}\right) = 3p^2(1-p) $$
%     $$ P\left(\overline{X}=1\right) = p^3 $$


% \end{exercise}

% \begin{exercise}
%     $  $

%     Dada una m.a.s $ (X_1,X_2,...,X_n)  $ de $ X \sim Exp(d) $, obtener la distribución en el muestreo del estadístico $ S = \sum\limits_{j=1}^{n}X_j $. Intentamos ver si Exp es reproductiva:
%     $$ \phi_{S}(t) = \phi_{X}(t)^{n} = \left( \left( 1-\dfrac{it}{\alpha}    \right)^{-1} \right)^{n} = \left( 1-\dfrac{it}{\alpha} \right)^{-n} $$

%     No lo es, pero si lo comparamos con la distribución Gamma, obtenemos que $ Exp(\alpha) \equiv \gamma(a = \alpha, p=1) $

%     Vamos a ver si la distribución Gamma es reproductiva respecto a algún parámetro.

%     $$ \phi_{S}(t) = \left( 1-\dfrac{it}{a} \right)^{-np} \implies S \sim \gamma(a,np) $$

%     Luego Gamma es reproductiva, entonces tenemos que el estadístico es:
%     $$ S = \sum\limits_{j=1}^{n}X_j \sim \gamma(\alpha, n) $$
% \end{exercise}


% \begin{exercise}
%     $  $

%     $$ \phi_{X}(t) = e^{it\mu-t^2\dfrac{\sigma^2}{2}} $$

%     $$ S = \sum\limits_{j=1}^{n}X_j \implies \phi_{S}(t) = \prod_{j=1}^{n}\phi_{X_j}(t) = \phi_{X}(t)^{n} = \left( e^{it\mu-t^2\dfrac{\sigma^2}{2}} \right)^{n} = e^{it\mu n-t^2 \dfrac{\sigma^2n}{2}} $$

%     Luego tenemos $ S \sim N(n\mu,n\sigma^2) $

%     Si queremos sacar el estadístico $ \overline{X}=\dfrac{S}{n} $ haremos:
%     $$ \phi_{\overline{X}}(t) = E(e^{it\overline{X}}) = E(e^{itS/n}) = \phi_{S}\left(\dfrac{t}{n}\right) = e^{it\mu-\dfrac{t^2\sigma^2n}{2n^2}}  $$
%     $$ \overline{X}\sim N\left(t,\dfrac{\sigma^2}{n}\right) $$
% \end{exercise}

% \begin{exercise}
%     $  $

%     Obtenemos primero la distribución de $ Y = X^2 $ utilizando cambio de variable:
%     $$ x(y) = \sqrt{y}\hspace{5mm}x'(y) = \dfrac{1}{2}y^{-1/2} $$
%     $$ f_{Y}(y) = \cancel{2y^{1/2}}e^{-y/\theta}\cancel{\dfrac{1}{2}y^{-1/2}}I_{(0,+\infty)}(y)=\dfrac{1}{\theta}e^{-y/\theta}I_{(0,+\infty)} \implies$$
%     $$ \implies Y \sim Exp\left(\dfrac{1}{\theta}\right) \equiv \gamma\left(\dfrac{1}{\theta},1\right) $$

%     Para ver la media y la varianza basta consultarlo en la hoja para la distribución Gamma.
% \end{exercise}

% \begin{proposition}
%     Para la media muestral tenemos:
%     $$ E(\overline{X}) = E(X) $$
% \end{proposition}
% \begin{demonstration}
%  $$   E(\overline{X}) = E(\sum\limits_{j=1}^{n}\dfrac{X_j}{n}) = \dfrac{1}{n}\sum\limits_{j=1}^{n}E(X_j) = \dfrac{1}{n}nE(X) = E(X)$$
% \end{demonstration}

% \begin{proposition}
%     Para la media muestral tenemos:
%     $$ Var(\overline{X}) = \dfrac{Var(X)}{n} $$
% \end{proposition}

% \begin{demonstration}
%     $$ Var(\overline{X}) = Var\left(\sum\limits_{j=1}^{n}X_j \right) = \dfrac{1}{n^2}\sum\limits_{j=1}^{n}Var(X_j) = \dfrac{1}{n^2}nVar(X) = \dfrac{Var(X)}{n} $$
% \end{demonstration}

% \begin{proposition}
%     $$ \overline{X} \xrightarrow{P}E(X) $$

% \end{proposition}
% \begin{demonstration}
%     Recordemos que decimos que una sucesión converge en probabilidad si se cumple una de las siguientes condiciones equivalentes:
%     $$ \forall \varepsilon >0, \lim_{n \to \infty} P(|X_n-X|\leq  \varepsilon) = 1 $$
%     $$ \forall \varepsilon >0, \lim_{n \to \infty} P(|X_n-X|\geq  \varepsilon) = 0 $$

%     Usaremos también la desigualdad de Tchebychev:
%     $$ \forall  \varepsilon>0, P(|\overline{X}-E(\overline{X})|\geq \varepsilon) \leq  \dfrac{Var(\overline{X})}{\varepsilon^2} $$
    
%     Aplicándolo a nuestro caso:
%     $$ \forall  \varepsilon>0, P(|\overline{X}-E(X)|\geq \varepsilon) \leq  \dfrac{Var(X)}{n\varepsilon^2} \to 0 $$


% \end{demonstration}

% \begin{proposition}
%     $$ \dfrac{\overline{X}-E(X)}{\sqrt{\dfrac{Var(X)}{n}}}\to_{d}N(0,1) $$
% \end{proposition}

% \begin{demonstration}
%     Basta usar el teorema central del límite que repasamos en el Tema 1.
% \end{demonstration}

% \begin{proposition}
%     \textbf{Propiedades de la proporción muestral}

%     \begin{enumerate}
%         \item $ \sum\limits_{j=1}^{n}X_j = n\widetilde{p} \sim B(n,p) $
%         \item $ E(\widetilde{p}) = p $
%         \item $ Var(\widetilde{p}) = \dfrac{p(1-p)}{n} $
%         \item $ \widetilde{p}\to_{p}p $
%         \item $ \dfrac{\widetilde{p}-p}{\sqrt{\dfrac{p(1-p)}{n}}} \to_{d}N(0,1) $

%     \end{enumerate}

% \end{proposition}
% \begin{demonstration}
%     Todas se derivan de la Observación 1 del guion.
% \end{demonstration}

% \textbf{Observación sobre la funcion de distribución empírica}

% Si $ A = \{X_j \leq  n\} $
% $$ P(A) = P(X_j\leq n ) = P(X\leq n) = F(n) $$

% Con esta observación demostramos las siguientes propiedades:


\section{Teorema de Wald}

 \subsection*{Consistencia}
 Sea $ X \sim F(\cdot ,\theta),\ \theta \in \Theta, \hat{\theta}_n $ estimador de $ \theta\ \forall n \in \mathbb{N} $, definimos las siguientes operaciones:
 $$ P_{\theta_0}(\hat{\theta}_n \in A ) := P(\hat{\theta}_n\in A| \theta = \theta_0) $$
$$ E_{\theta_0}[\hat{\theta}_n] = \idotsint\limits_{\psi}^{}\hat{\theta}_n(x)L(x,\theta_0)dx $$

Se dice que $ \hat{\theta}_n $ es consistente para $ \theta \in \Theta $ si $ \hat{\theta}_n\xrightarrow{P_{\theta_0}} \theta_0\ \text{(convergencia en probabilidad)},\ \forall \theta_0 \in \Theta $

% \begin{flushright}
%     \textbf{Observación}
% \end{flushright}

\begin{proposition}
    Si $ \Theta $ es finito y $ \hat{\theta}_n $ es estimador de $ \theta $ entonces se da la consistencia del estadístico si y solo si:
    $$ \lim_{n \to \infty}P_{\theta_0}(\hat{\theta}_n=\theta_0)=1,\ \forall \theta_0 \in \Theta $$
    
    Es decir,
    $$ \hat{\theta}_{n} \xrightarrow{P_{\theta_0}} \theta_0\ \forall \theta \in \Theta \iff \lim_{n \to \infty}P_{\theta_0}(\hat{\theta}_{n}=\theta_0 )=1$$
    
\end{proposition}

\begin{demonstration}
    $  $\\
    $ \impliedby $

    Trivial.\\
    $ \implies $

    Si $ \hat{\theta}_{n} $ es el EMV de $ \theta $, tomará un valor aislado dentro de $ \Theta $.
    
    Si $ \hat{\theta}_{n} \xrightarrow{P_{\theta_0}} \theta_0 $, tomando $ \varepsilon $ suficientemente pequeño, $ \hat{\theta}_{n} $ solo puede tomar el valor de $ \theta_0   $, luego $ \lim_{n \to \infty}P_{\theta_0}(\hat{\theta}_{n} = \theta_0) = 1 $


\end{demonstration}

\begin{proposition}
    \textbf{Desigualdad de Jensen}

    Sea $ X $ v.a. y $ g $ una función cóncava, entonces $ E[g(X)]<g(E[X]) $ siempre que las esperanzas anteriores existan.
\end{proposition}

\begin{proposition}
    \textbf{Ley fuerte de Kolmogorov}

    Sea $ \{X_n\}_{n=1}^{\infty} $ sucesión de v.v.a.a. independientes, idénticamente distribuidas y con media finita. Entonces:
    $$ \sum\limits_{i=1}^{n} \dfrac{X_i}{n} \xrightarrow{c.s.} E[X] $$
\end{proposition}

\begin{lemma}
    Sean $ \{A_n\},\ \{B_n\} $ con $ \lim_{n \to \infty}\{P(A_n)\} = \lim_{n \to \infty} \{P(B_n)\} = 1 $, entonces:
    $$ \lim_{n \to \infty} P(A_n \cap B_n) = 1 $$
\end{lemma}

\begin{demonstration}
    $$ P(A_n \cap B_n) = P(A_n) + P(B_n) - P(A_n \cup B_n) $$
    
    Pero tenemos:
    $$ 1 \geq\ P(A_n\cup B_n) \geq P(A_n) \to 1 $$
    
    Luego $ \lim_{n \to \infty}P(A_n \cap B_n) = 1 $ y tenemos que:
    $$ P(A_n \cap B_n) = P(A_n) + P(B_n) - P(A_n \cup B_n) = 1 +1-1 =1 $$

\end{demonstration}


\begin{theorem}
    Sea $ X $ variable aleatoria con función de distribución $ F(\cdot ,\theta) $ para $ \theta \in \Theta $, siendo $ \Theta $ un conjunto finito. Supongamos que se verifica:
    \begin{itemize}
        \item (A1) El soporte de $ F(\cdot ,\theta) $ es común para todo $ \theta \in \Theta $.
        \item (A2) $ E_{\theta_0}\left[\log{{f}({X,\theta})}\right] $ existe y es finita para todo $ \theta,\ \theta_0 \in \Theta$
        
    \end{itemize}
    Si para todo $ n \in \mathbb{N} $ y $ (X_1,...,X_n) $ m.a.s de $ X $ existe el $ \operatorname{EMV}(\hat{\theta}_{n}) $ de $ \theta $ y es único entonces $ \hat{\theta}_{n} $ es un estimador consistente del parámetro $ \theta $.
\end{theorem}
\begin{demonstration}
    Utilizaremos la Desigualdad de Jensen, tomaremos $ g = \log{} $ y la v.a. $ \dfrac{f(X,\theta)}{f(X,\theta_0)} $. La Desigualdad de Jensen entonces nos dice:
    $$ E_{\theta_0}\left[\log{\left(\dfrac{f(X,\theta)}{f(X,\theta_0)}\right)}\right] < \log{\left(  E_{\theta_0}\left[\dfrac{f(X,\theta)}{f(X,\theta_0)}\right]\right)} $$

    Pero notemos que:
    $$   E_{\theta_0}\left[\dfrac{f(X,\theta)}{f(X,\theta_0)}\right] = \int\limits_{}^{} \dfrac{f(x,\theta)}{f(x,\theta_0)}f(x,\theta_0)dx =_{(A1)} 1 $$

    Luego tenemos que:
    $$ E_{\theta_0}\left[\log{\left(\dfrac{f(X,\theta)}{f(X,\theta_0)}\right)}\right] < 0 $$

    Usaremos ahora la ley fuerte de Kolmogorov a la sucesión $ \left\{\log{\left(  \dfrac{f(X_n,\theta)}{f(X_n,\theta_0)}\right)}\right\}_{n} $ y utilizando que la convergencia casi segura es más fuerte que la convergencia en probabilidad, tenemos :
    $$\dfrac{\log{\left( \dfrac{L(\mathbb{X},\theta)}{L(\mathbb{X},\theta_0)} \right)}}{n} =  \sum\limits_{i=1}^{n} \dfrac{\log{\left( \dfrac{f(x_i,\theta)}{f(x_i,\theta_0)} \right)}}{n} \xrightarrow{P_{\theta_0}} E_{\theta_0}\left[\log{\left(\dfrac{f(X,\theta)}{f(X,\theta_0)}\right)}\right] < 0 $$

    Como a partir de cierto $ n $ el logaritmo de la izquierda será negativo podemos tomar:
    $$ \lim_{n \to \infty}P_{\theta_0} \left( \log{ \dfrac{L(\mathbb{X},\theta)}{L(\mathbb{X},\theta_0  )}}<0 \right) = \lim_{n \to \infty}P_{\theta_0} (L(\mathbb{X},\theta)<L(\mathbb{X},\theta_0)) = 1$$

    Como el espacio paramétrico es finito, utilizaremos la equivalencia para la definición de consistencia y buscaremos el límite:
    $$ \lim_{n \to \infty} P_{\theta_0}(\hat{\theta}_{n} = \theta_0 ) $$

    Estudiamos ahora el suceso $ \{\hat{\theta}_{n}(x) = \theta_0\} = \bigcap _{\substack{\theta \in \Theta\\\theta\ne \theta_0}} \{L(\mathbb{X},\theta) < L(X,\theta_0)\} $ ya que $ \widetilde{\theta}_{n}$ es EMV único.

    El suceso es intersección finita de sucesos cuya probabilidad tienen límite 1. Utilizando el lema llegamos a:
    $$ \lim_{n \to \infty}P(\hat{\theta}_{n} = \theta_0 ) = 1 $$
\end{demonstration}


\begin{example}
    $ X \sim \mathcal{U}(\theta,\theta+1),\ \theta \in \mathbb{R},\ X_1,...,X_n$ m.a.s de $ X $, $ (f(x,\theta) = 1,\ si\ x \in (\theta,\theta+1)) $. Buscar el EMV de $ \theta $.

    $$ L(\mathbb{X},\theta) = 1\ si\ (x_i \in (\theta,\theta+1),\ i = 1,...,n) \iff (\theta < x_{1:n}\ y\ x_{n:n}< \theta+1) \iff (x_{n:n}-1 < \theta < x_{1:n}) $$

    En $ \alpha (x_{n:n}-1)+(1-\alpha)x_{1:n} $ se alcanza el máximo de $ L(\mathbb{X},\theta)\ \forall\ \alpha \in (0,1) $
\end{example}

\section{Método de la función pivote}

\setcounter{dummy}{0}

\begin{proposition}
    Sea $ X $ una variable aleatoria con función de distribución continua $ F(x,\theta),\ \theta \in \Theta  $ y monótona y sea $ X = (X_1,...,X_n) $
\end{proposition}

Para demostrarlo necesitamos algunos resultados

\begin{lemma}
    
    Sea $ X $ v.a. con función de distribución $ F $ continua, entonces $ F(X) \sim \mathcal{U}(0,1)$ 
\end{lemma}

\begin{lemma}
    Sea $ \mathcal{U} \sim \mathcal{U}(0,1) $, entonces $ -\log{U} \sim \operatorname{Exp}(1) $
\end{lemma}

Vamos ahora con la demostración de la proposición.

\begin{demonstration}
    Tenemos que $ \{F(X) \leq  x\} \iff \{X \leq  F ^{-1}(x)\} $, entonces:
    $$ P(F(X)\leq x) = P(X \leq  F ^{-1}(x)) = F(F ^{-1}(x)) = x\  \forall x \in (0,1)$$
\end{demonstration}

\begin{demonstration}
    $ T(\mathbb{X},\theta) = - \sum\limits_{j=1}^{n} \log{F(X_j,\theta)} $ trivialmente es monótona en $ \theta $ (al serlo $ F $)

    Vemos ahora que $ T(\mathbb{X}, \theta) $ no depende de $ \theta $ en su distribución.

    $$ T(\mathbb{X},\theta) = \sum\limits_{j=1}^{n} E_j $$

    Donde $ E_j $ son $ \operatorname{Exp}(1) $ indep. entre sí. Pero sabemos que la suma de exponenciales tiene una distribución Gamma:
    $$ T(\mathbb{X},\theta) \sim \Gamma(1,n) $$

    Luego no depende de $ \theta $.

\end{demonstration}


\section*{Ejemplo del teorema de Newman-Pearson}

\begin{example}
    Sea $ X \sim N(\mu, \theta^2) $ con $ \theta^2 $ conocida. Se considera una m.a.s simple de tamaño $ n $ de $ X $. Obtener el test de máxima potencia y extensión de $ \alpha $ para el test (contraste):
    $$ H_0: \mu = \mu_0 $$

    frente a:
    $$ H_1: \mu = \mu_1 $$

    Aplicamos el teorema de Neyman-Pearson. Tenemos que el test UMP viene dado por:
    $$ S_1 = \{x \in \operatorname{Sop}(X):  \dfrac{L(x,\mu_1)}{L(x,\mu_0)}\geq  k \} $$
    
    Donde $ k $ verifica que $ \alpha = P_{\theta_1}(X \in S) $
    $$ L(x,\mu) = \left( \dfrac{1}{\sigma \sqrt{2\pi}}^{n} e^{\dfrac{-1}{2}\dfrac{\sum (x_i-\mu)^2}{\sigma^2}} \right) = \left( \dfrac{1}{\sigma\sqrt{2\pi}}^{n}e^{-\dfrac{1}{2} \dfrac{\sum x_i^2-2\mu\sum x_i+n\mu}{\sigma^2}}  \right) \to \dfrac{L(X,\mu_1)}{L(X,\mu_0)} = $$ 
    $$ =exp \{-\dfrac{1}{2\sigma ^2}(2(\mu_0-\mu_1)\sum x_i+n(\mu_1^2-\mu_0^2))\} $$
    $$ S_1 = \{x \in \operatorname{Sop}(X):\ -\dfrac{1}{2\sigma ^2}(2(\mu_0-\mu_1)\sum x_i + n(\mu_1^2-\mu_0^2)) \geq  k' = \log{k}\} =$$ 
    
    Suponiendo $ \mu_1>\mu_0 $
    $$ \{x \in \operatorname{Sop}(X):\ \sum x_i \leq  \dfrac{-2\sigma^2k'-n(\mu_1^2-\mu_0^2)}{2(\mu_1-\mu_0)} \} = \{x \in \operatorname{Sop}(X):\ \dfrac{\sum x_i}{n} \leq  \underbrace{\dfrac{-2\sigma^2k'-n(\mu_1^2-\mu_0^2)}{2n(\mu_1-\mu_0)}}_{=:k''} \} $$

    Fijando $ \alpha $, tendremos que buscar $ k''(\alpha) $. Sabemos que $ \overline{X} \sim N\left(\mu,\dfrac{\sigma}{\sqrt{n}}\right) $

    $$ \alpha = P_{\mu_0}\left(\overline{X} \geq  k''(\alpha)\right) = P_{\mu_0} \left(\underbrace{\dfrac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}}_{Z \sim N(0,1)} \geq  \dfrac{k''(\alpha)-\mu_0}{\sigma/\sqrt{n}}\right) \to \dfrac{k''(\alpha)-\mu_0}{\sigma/\sqrt{n}} = Z_{1-\alpha} $$

    Luego $ k''(\alpha) = \mu_0 + Z_{1-\alpha} \dfrac{\sigma}{\sqrt{n}} $

    La probabilidad de error de tipo 2 es:

    $$ \beta = P_{\mu_1} (\overline{X} < \mu_0+Z_{1-\alpha}\dfrac{\sigma}{\sqrt{n}}) = P \left( \dfrac{\overline{X}-\mu_1}{\sigma/\sqrt{n}}< \dfrac{\mu_0-\mu_1+Z_{1-\alpha}\dfrac{\sigma}{\sqrt{n}}}{\sigma/\sqrt{n}}\right) = P\left(Z < \dfrac{\mu_0\mu_1}{\sigma/\sqrt{n}}+Z_{1-\alpha}\right)  $$

    Entonces:
    $$ Z_{\beta} = \dfrac{k''(\alpha)-\mu_1}{\dfrac{\sigma}{\sqrt{n}}} $$

    $$ \mu_0+Z_{1-\alpha} \dfrac{\sigma}{\sqrt{n}} = \mu_1 + Z_{\beta} \dfrac{\sigma}{\sqrt{n}} \implies \dfrac{\mu_1\mu_0}{Z_{1-\alpha}-Z_{\beta}} = \dfrac{\sigma}{\sqrt{n}} \implies n = \left( \dfrac{\sigma (Z_{1-\alpha}-Z_{\beta})}{\mu_1-\mu_0} \right)^2 $$



    
\end{example}


\begin{example}
    Si $ X \sim N (\mu,\sigma^2 = 4)  $ representa la duración en días de una determinada enfermedad y se considera la duración de una muestra de 9 enfermos resultando en días: 5,3,4,2,6,4,5,3,4. Aplicar los resulados del ejemplo anterior para decidir entre:
    $$ H_0: \mu = 3  $$

    frente a:

    $$ H_1: \mu = 4 $$

    con una extensión de $ \alpha = 0.05  $

    En primer lugar tenemos que nuestra región de rechazo es:
    $$ S_1 = \left\{x \in \mathbb{R}^{9}:\ \overline{X}\geq 3+Z_{0.95} \dfrac{2}{3}\right\} = \{x \in \mathbb{R}^{9}:\ \overline{X}\geq 4.1\} $$

    Además tenemos que en este caso $ \overline{X} = 4 $. Luego como $ 4 \not \geq 4.1 $ aceptamos $ H_0 $, es decir, que $ \mu=3 $.
\end{example}

\begin{proposition}
    Si $ A(\theta) $ es monótona creciente (decreciente).

    a) En el caso $ \theta_0<\theta $:
    $$ \delta(x) = \left\{
    \begin{array}{ll}
        0 & T(x) < (>) c \\ 
        1 & T(x) \geq  (\leq ) c
    \end{array}
    \right. $$
    
    b) En el caso $ \theta_0>\theta $:
    $$ \delta(x) = \left\{
    \begin{array}{ll}
        0 & T(x) > (<) c \\ 
        1 & T(x) \leq  (\geq ) c
    \end{array}
    \right. $$

\end{proposition}

\begin{demonstration}
    Si tenemos $ L(x,\theta) = exp \{A(\theta)T(\theta)+B(\theta)+h(x)\} $

    $$ \dfrac{L(x,\theta_1)}{L(x,\theta_0)} = exp \{(A(\theta_1)A(\theta_0))T + (B(\theta_1)-B(\theta_0))\} $$

    Entonces si escribimos $ S_1 $ como en el teorema de Neyman-Pearson y despejamos:
    $$ S_1 = \{x \in \operatorname{Sop}(X):\ T \geq (\leq ) \dfrac{k'-(B(\theta_1)-B(\theta_0))}{A(\theta_1)A(\theta_0)}\} $$

    La desigualdad depende del signo de $ A(\theta_1)A(\theta_0) $ (o sea, si es creciente o decreciente).
\end{demonstration}

\begin{example}
    Sea $ X \sim \operatorname{Exp}(\theta) $. Dada una m.a.s. de tamaño $ n $, obtener el test de máxima potencia y extensión $ \alpha $ para el test (contraste)
        $$ H_0: \theta = \theta_0 $$
    frente a:
    $$ H_1: \theta = \theta_1 $$
    
    En este caso, $ A(\theta) = -\theta $ y $ T = \sum\limits_{i=1}^{n}X_i $. Luego $ A $ es decreciente. Supongamos $ \theta_0<\theta_1 $, entonces:
    $$ S_1 = \{x \in R_{+}^{n}:\ \sum\limits_{i=1}^{n} X_i\leq  c\} $$
    $$ \alpha = P_{\theta=\theta_0}(X \in S_1) = P_{\theta_0}\left(\sum\limits_{}^{}X_i \leq  c\right)  $$

    En el tema 2 obtuvimos que $ \sum X_i \sim \Gamma() $ . Si llamamos a la función cuantil de $ \Gamma $, $ G $ entonces $ c = G_{\alpha} $
    
\end{example}

\section{Ejemplo de familia con cociente de verosimilitud monótono}

\begin{example}
    $ $

    $ X \sim B(p)$

    $$ L(x,p) = \prod_{j}^{} p^{x_j} (1-p)^{x_j} = p^{\sum\limits_{}^{}x_j} (1-p)^{n-\sum\limits_{}^{}x_j} $$

    Sean $ p_0<p_1 \in [0,1]$:
    $$ \dfrac{L(X,p_1)}{L(X,p_2)} = \dfrac{p_1^{\sum\limits_{}^{}x_j}(1-p_1)^{n-\sum x_j}}{p_0^{\sum x_j}(1-p_0)^{n-\sum x_j}} = \underbrace{\left( \dfrac{p_1(1-p_0)}{(1-p_1)(p_0)}^{\sum x_j} \right)}_{c>1} \left( \dfrac{1-p_1}{1-p_0} \right)^{n} = $$  
    $$=c ^{\sum x_j} \left( \dfrac{1-p_1}{1-p_0} \right)^{n} = h(\sum x_j) \text{ creciente} $$
\end{example}

\newpage
\section{Modelo lineal general. Aplicaciones y ejercicios}



\end{document}


